### 外包项目：微信公众号爬虫

### 采集目标：文章阅读量，点赞量，在看量，文章发布日期，文章标题等



### 正式开始

```stylus
流程分析：
	1.进入微信公众平台，通过发布图文动态，引入其他公众号文章的方式获取采集公众号文章的json数据
	2.在json数据中，可以获取公众号文章的访问链接，将参数部分进行分离，构造请求参数params字典
	3.使用fiddler抓包，获取微信身份权重key值和uin值
	4.使用postman或者fiddler的composer功能，筛选过滤出请求参数data
	5.对阅读量等数据异步加载的地址发送请求，获取对应的响应数据即可完成采集目标
```

###代码展示

```python
# -*- coding: utf-8 -*-
# @Author : 阿尔法
# @File : demo.py
# @Software: PyCharm
from requests_html import HTMLSession
# 构造请求对象
session = HTMLSession()
import time
"""会发生变化的参数：变化周期：10分钟左右"""
# 浏览器上获取
fakeid = 'MjM5Njc4ODY1MQ=='
token = '1123958135'
cookie = 'ua_id=Zki2ZL6cfwnT6qybAAAAAO4ek_qtIUf6Vh_ftQhMCrc=; wxuin=60886836261131; uuid=f5d538cf0725743fc3f633e2afa9cb81; rand_info=CAESICT/DefsJnttX8DbJ8btiB4uKQ/Q99MQGerUSkf0REzH; slave_bizuin=3535305560; data_bizuin=3535305560; bizuin=3535305560; data_ticket=gxWnHMwPfxfLmGe2jxLws6ZluyyN3KX0/aqXCmCY/lHhz11JlUUuv0vZ01RQV5J/; slave_sid=bzZyWnU3bGUyU1hTbkttS0Jaak9aN1dQZlAyX0lraFBTNnh4RjdsSk5RdmNCVmttU2ZUeTlWSW5hZUhWbHdvUzNSWW1MYzVGRzFSdFpLTEl3amUxVzMzZlpUdlVUb0RUbUlSUHVjcWhZNHpFazRNUFM2Y1FtWlNRbGpVNEs2VVpkRng5N0lDWE93TXRYdU1k; slave_user=gh_971cf9c7e209; xid=1b85ec1808c0d19ffe61825d7fc207cb; mm_lang=zh_CN'
# fiddler上面获取
uin = 'MjgxMjM1MjAxOA=='
key = 'f5ef2f57f587c3ebd8eddd64bcdf69c53257522811696cf1d17844cc193230efa736111220e4c65ad0d9528fcb3fad165fbcf4e4727c8fa107ca1dc8a4298d509872b45b4a4ce7d8b5b04942d10868a889ac874ec02f4ba8122544b5f11ee7d171397b404b7f061d2a165bf01bd7252c65b9450a7f6d82f6620028a3578436d9'


class WXSpider(object):

    def __init__(self):
        """
        # 准备数据
        """
        # 微信公众号查询地址
        self.start_url = 'https://mp.weixin.qq.com/cgi-bin/appmsg'
        # 微信公众号的headers
        self.headers = {
            'cookie': cookie,
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'
        }
        # fiddler上准备的数据
        self.search_url = 'https://mp.weixin.qq.com/mp/getappmsgext'

    def parse_start_url(self):
        """
        解析微信公众号PC端查询结果
        :return:
        """
        for page in range(10):
            params = {
                'action': 'list_ex',
                'fakeid': fakeid,
                'query': '',
                'begin': str(page * 4),
                'count': '4',
                'type': '9',
                'need_author_name': '1',
                'token': token,
                'lang': 'zh_CN',
                'f': 'json',
                'ajax': '1'
            }
            # 发送请求，获取结果
            response = session.get(self.start_url, headers=self.headers, params=params).json()
            self.parse_response_data(response)
            time.sleep(5)

    def parse_response_data(self, response):
        """
        解析查询结果的响应
        :param response: json数据
        :return:
        """
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36 NetType/WIFI MicroMessenger/7.0.20.1781(0x6700143B) WindowsWechat(0x63070517)',
        }
        # 提取结果列表
        app_msg_list = response['app_msg_list']
        for app_msg in app_msg_list:
            # 字符串拆分partition，以partition的规则，将字符串拆分为三个部分
            link = app_msg['link'].partition('?')[2].split('&')
            biz = link[0].partition('=')[2]
            mid = link[1].partition('=')[2]
            idx = link[2].partition('=')[2]
            sn = link[3].partition('=')[2]
            """构造请求的params参数"""
            params = {
                'uin': uin,
                'key': key,
                '__biz': biz,
            }
            """请求体查询参数"""
            data = {
                'mid': mid,
                'sn': sn,
                'idx': idx,
                'is_only_read': '1',
            }
            # 发送请求，获取观看量，点赞量，在看量等数据
            response = session.post(self.search_url, headers=headers, params=params, data=data).json()
            print(response['appmsgstat']['read_num'])



if __name__ == '__main__':
    w = WXSpider()
    w.parse_start_url()
```


